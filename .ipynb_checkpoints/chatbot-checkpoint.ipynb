{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import tensorflow\n",
    "import tflearn\n",
    "import numpy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intents.json\", 'rb') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intents': [{'tag': 'cumprimento',\n",
       "   'patterns': ['Olá', 'Ei', 'Oi', 'Bom dia', 'Boa noite'],\n",
       "   'responses': ['Olá, bem-vindo ao nosso Chatbot',\n",
       "    'Bom ver você, bem-vindo ao nosso chatbot',\n",
       "    'Olá, como posso ajudar?'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'adeus',\n",
       "   'patterns': ['Tchau',\n",
       "    'Até logo',\n",
       "    'Bom bate-papo com você, tchau',\n",
       "    'Até a próxima'],\n",
       "   'responses': ['Até logo!',\n",
       "    'Tenha um bom dia',\n",
       "    'Tchau! Volte logo.',\n",
       "    'Feliz por ajudar'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'obrigado',\n",
       "   'patterns': ['obrigado',\n",
       "    'Isso ajuda',\n",
       "    'Demais obrigado',\n",
       "    'Obrigado por me ajudar'],\n",
       "   'responses': ['Feliz por ajudar! ',\n",
       "    'A qualquer momento! Posso ajudar em algum outro problema?',\n",
       "    'O prazer é meu! Posso ajudar com algum outro problema?'],\n",
       "   'context': ['']},\n",
       "  {'tag': 'valor do IPVA',\n",
       "   'patterns': ['Qual o valor do IPVA?'],\n",
       "   'responses': ['Os valores separados do IPVA, da Taxa de Licenciamento Anual e do seguro obrigatório podem ser obtidos nesta página da Internet (www.receita.fazenda.df.gov.br) em: Menu Receita / Serviços Cidadão / Veículos, mediante o fornecimento do nº do RENAVAM ou por meio do telefone “156”, opção “3”, ou em uma das unidades do “Na Hora”, ou em uma das Agências/Posto de Atendimento da Receita, mediante o fornecimento do nº da placa e do RENAVAM.'],\n",
       "   'context': ['ipva']},\n",
       "  {'tag': 'valores pagos e a pagar do IPVA',\n",
       "   'patterns': ['Como pagar o IPVA dos exercícios anteriores?'],\n",
       "   'responses': ['Os valores pagos e a pagar podem ser obtidos nesta página da Internet (www.receita.fazenda.df.gov.br) em: Menu Receita / Serviços Cidadão / Veículos, ou por meio do telefone “156”, opção “3”, mediante o fornecimento do nº da placa e do RENAVAM, ou em uma das Agências/Posto de Atendimento da Receita, por meio da indicação do RENAVAM ou documentos que comprovem ser o proprietário do veículo ou procurador deste (CRLV, documento de identidade, CPF e procuração, se for o caso).'],\n",
       "   'context': ['ipva']}]}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_size = len(words)\n",
    "words_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_empty = [0 for _ in range(len(labels))]\n",
    "for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = numpy.array(training)\n",
    "output = numpy.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.reset_default_graph()\n",
    "tflearn.init_graph(num_cores=4,gpu_memory_fraction=0.5)\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.embedding(net, input_dim=words_size, output_dim=128)\n",
    "#net = tflearn.lstm(net, 128, dropout=0.8, return_seq=True)\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: intents\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 15\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 11.018s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 15/15\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m1.44952\u001b[0m\u001b[0m | time: 0.223s\n",
      "| Adam | epoch: 002 | loss: 1.44952 - acc: 0.0000 -- iter: 15/15\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m1.57844\u001b[0m\u001b[0m | time: 0.169s\n",
      "| Adam | epoch: 003 | loss: 1.57844 - acc: 0.2727 -- iter: 15/15\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m1.59756\u001b[0m\u001b[0m | time: 0.153s\n",
      "| Adam | epoch: 004 | loss: 1.59756 - acc: 0.3182 -- iter: 15/15\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.59914\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 005 | loss: 1.59914 - acc: 0.3287 -- iter: 15/15\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.59498\u001b[0m\u001b[0m | time: 0.135s\n",
      "| Adam | epoch: 006 | loss: 1.59498 - acc: 0.3317 -- iter: 15/15\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m1.58783\u001b[0m\u001b[0m | time: 0.121s\n",
      "| Adam | epoch: 007 | loss: 1.58783 - acc: 0.3327 -- iter: 15/15\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.57623\u001b[0m\u001b[0m | time: 0.106s\n",
      "| Adam | epoch: 008 | loss: 1.57623 - acc: 0.3705 -- iter: 15/15\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.55878\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 009 | loss: 1.55878 - acc: 0.3508 -- iter: 15/15\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m1.52746\u001b[0m\u001b[0m | time: 0.100s\n",
      "| Adam | epoch: 010 | loss: 1.52746 - acc: 0.3421 -- iter: 15/15\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m1.49422\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 011 | loss: 1.49422 - acc: 0.3379 -- iter: 15/15\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m1.46637\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 012 | loss: 1.46637 - acc: 0.3359 -- iter: 15/15\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.45367\u001b[0m\u001b[0m | time: 0.249s\n",
      "| Adam | epoch: 013 | loss: 1.45367 - acc: 0.3348 -- iter: 15/15\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.44246\u001b[0m\u001b[0m | time: 0.118s\n",
      "| Adam | epoch: 014 | loss: 1.44246 - acc: 0.3342 -- iter: 15/15\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.43520\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 015 | loss: 1.43520 - acc: 0.3339 -- iter: 15/15\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.43337\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 016 | loss: 1.43337 - acc: 0.3337 -- iter: 15/15\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.43250\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 017 | loss: 1.43250 - acc: 0.3335 -- iter: 15/15\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.43718\u001b[0m\u001b[0m | time: 0.116s\n",
      "| Adam | epoch: 018 | loss: 1.43718 - acc: 0.3335 -- iter: 15/15\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m1.43882\u001b[0m\u001b[0m | time: 0.106s\n",
      "| Adam | epoch: 019 | loss: 1.43882 - acc: 0.3334 -- iter: 15/15\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m1.43011\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 020 | loss: 1.43011 - acc: 0.3334 -- iter: 15/15\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m1.43227\u001b[0m\u001b[0m | time: 0.116s\n",
      "| Adam | epoch: 021 | loss: 1.43227 - acc: 0.3334 -- iter: 15/15\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m1.43556\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 022 | loss: 1.43556 - acc: 0.3334 -- iter: 15/15\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m1.43747\u001b[0m\u001b[0m | time: 0.125s\n",
      "| Adam | epoch: 023 | loss: 1.43747 - acc: 0.3334 -- iter: 15/15\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m1.43430\u001b[0m\u001b[0m | time: 0.118s\n",
      "| Adam | epoch: 024 | loss: 1.43430 - acc: 0.3146 -- iter: 15/15\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m1.43148\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 025 | loss: 1.43148 - acc: 0.3197 -- iter: 15/15\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m1.43517\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 026 | loss: 1.43517 - acc: 0.3057 -- iter: 15/15\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.43663\u001b[0m\u001b[0m | time: 0.097s\n",
      "| Adam | epoch: 027 | loss: 1.43663 - acc: 0.3128 -- iter: 15/15\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.43591\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 028 | loss: 1.43591 - acc: 0.3179 -- iter: 15/15\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.43279\u001b[0m\u001b[0m | time: 0.106s\n",
      "| Adam | epoch: 029 | loss: 1.43279 - acc: 0.3217 -- iter: 15/15\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m1.43211\u001b[0m\u001b[0m | time: 0.406s\n",
      "| Adam | epoch: 030 | loss: 1.43211 - acc: 0.3244 -- iter: 15/15\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.43804\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 031 | loss: 1.43804 - acc: 0.3265 -- iter: 15/15\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m1.43319\u001b[0m\u001b[0m | time: 0.100s\n",
      "| Adam | epoch: 032 | loss: 1.43319 - acc: 0.3280 -- iter: 15/15\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m1.43434\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 033 | loss: 1.43434 - acc: 0.3292 -- iter: 15/15\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m1.43365\u001b[0m\u001b[0m | time: 0.117s\n",
      "| Adam | epoch: 034 | loss: 1.43365 - acc: 0.3301 -- iter: 15/15\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m1.43468\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 035 | loss: 1.43468 - acc: 0.3308 -- iter: 15/15\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m1.43483\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 036 | loss: 1.43483 - acc: 0.3313 -- iter: 15/15\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m1.43338\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 037 | loss: 1.43338 - acc: 0.3317 -- iter: 15/15\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m1.43370\u001b[0m\u001b[0m | time: 0.113s\n",
      "| Adam | epoch: 038 | loss: 1.43370 - acc: 0.3320 -- iter: 15/15\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m1.43579\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 039 | loss: 1.43579 - acc: 0.3323 -- iter: 15/15\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m1.42962\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 040 | loss: 1.42962 - acc: 0.3325 -- iter: 15/15\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m1.42899\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 041 | loss: 1.42899 - acc: 0.3326 -- iter: 15/15\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m1.42962\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 042 | loss: 1.42962 - acc: 0.3328 -- iter: 15/15\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m1.43290\u001b[0m\u001b[0m | time: 0.088s\n",
      "| Adam | epoch: 043 | loss: 1.43290 - acc: 0.3329 -- iter: 15/15\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.42883\u001b[0m\u001b[0m | time: 0.139s\n",
      "| Adam | epoch: 044 | loss: 1.42883 - acc: 0.3329 -- iter: 15/15\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.42648\u001b[0m\u001b[0m | time: 0.125s\n",
      "| Adam | epoch: 045 | loss: 1.42648 - acc: 0.3330 -- iter: 15/15\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.42881\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 046 | loss: 1.42881 - acc: 0.3331 -- iter: 15/15\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.42876\u001b[0m\u001b[0m | time: 0.124s\n",
      "| Adam | epoch: 047 | loss: 1.42876 - acc: 0.3331 -- iter: 15/15\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.43262\u001b[0m\u001b[0m | time: 0.239s\n",
      "| Adam | epoch: 048 | loss: 1.43262 - acc: 0.3331 -- iter: 15/15\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.43019\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 049 | loss: 1.43019 - acc: 0.3332 -- iter: 15/15\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.43002\u001b[0m\u001b[0m | time: 0.118s\n",
      "| Adam | epoch: 050 | loss: 1.43002 - acc: 0.3435 -- iter: 15/15\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.43249\u001b[0m\u001b[0m | time: 0.120s\n",
      "| Adam | epoch: 051 | loss: 1.43249 - acc: 0.3318 -- iter: 15/15\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.43384\u001b[0m\u001b[0m | time: 0.143s\n",
      "| Adam | epoch: 052 | loss: 1.43384 - acc: 0.3320 -- iter: 15/15\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.43234\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 053 | loss: 1.43234 - acc: 0.3322 -- iter: 15/15\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.43253\u001b[0m\u001b[0m | time: 0.114s\n",
      "| Adam | epoch: 054 | loss: 1.43253 - acc: 0.3324 -- iter: 15/15\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.43602\u001b[0m\u001b[0m | time: 0.124s\n",
      "| Adam | epoch: 055 | loss: 1.43602 - acc: 0.3230 -- iter: 15/15\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.43569\u001b[0m\u001b[0m | time: 0.119s\n",
      "| Adam | epoch: 056 | loss: 1.43569 - acc: 0.3245 -- iter: 15/15\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.43554\u001b[0m\u001b[0m | time: 0.110s\n",
      "| Adam | epoch: 057 | loss: 1.43554 - acc: 0.3257 -- iter: 15/15\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.43286\u001b[0m\u001b[0m | time: 0.107s\n",
      "| Adam | epoch: 058 | loss: 1.43286 - acc: 0.3267 -- iter: 15/15\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.43132\u001b[0m\u001b[0m | time: 0.152s\n",
      "| Adam | epoch: 059 | loss: 1.43132 - acc: 0.3276 -- iter: 15/15\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.43104\u001b[0m\u001b[0m | time: 0.126s\n",
      "| Adam | epoch: 060 | loss: 1.43104 - acc: 0.3372 -- iter: 15/15\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.42996\u001b[0m\u001b[0m | time: 0.106s\n",
      "| Adam | epoch: 061 | loss: 1.42996 - acc: 0.3454 -- iter: 15/15\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.42760\u001b[0m\u001b[0m | time: 0.109s\n",
      "| Adam | epoch: 062 | loss: 1.42760 - acc: 0.3610 -- iter: 15/15\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m1.42688\u001b[0m\u001b[0m | time: 0.167s\n",
      "| Adam | epoch: 063 | loss: 1.42688 - acc: 0.3575 -- iter: 15/15\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m1.42718\u001b[0m\u001b[0m | time: 0.163s\n",
      "| Adam | epoch: 064 | loss: 1.42718 - acc: 0.3545 -- iter: 15/15\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.42531\u001b[0m\u001b[0m | time: 0.129s\n",
      "| Adam | epoch: 065 | loss: 1.42531 - acc: 0.3601 -- iter: 15/15\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m1.42120\u001b[0m\u001b[0m | time: 0.109s\n",
      "| Adam | epoch: 066 | loss: 1.42120 - acc: 0.3568 -- iter: 15/15\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m1.42109\u001b[0m\u001b[0m | time: 0.105s\n",
      "| Adam | epoch: 067 | loss: 1.42109 - acc: 0.3620 -- iter: 15/15\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m1.41790\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 068 | loss: 1.41790 - acc: 0.3586 -- iter: 15/15\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m1.41710\u001b[0m\u001b[0m | time: 0.110s\n",
      "| Adam | epoch: 069 | loss: 1.41710 - acc: 0.3557 -- iter: 15/15\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m1.41598\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 070 | loss: 1.41598 - acc: 0.3608 -- iter: 15/15\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m1.41552\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 071 | loss: 1.41552 - acc: 0.3576 -- iter: 15/15\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m1.41327\u001b[0m\u001b[0m | time: 0.110s\n",
      "| Adam | epoch: 072 | loss: 1.41327 - acc: 0.3624 -- iter: 15/15\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m1.40810\u001b[0m\u001b[0m | time: 0.109s\n",
      "| Adam | epoch: 073 | loss: 1.40810 - acc: 0.3666 -- iter: 15/15\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m1.41190\u001b[0m\u001b[0m | time: 0.121s\n",
      "| Adam | epoch: 074 | loss: 1.41190 - acc: 0.3556 -- iter: 15/15\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m1.41175\u001b[0m\u001b[0m | time: 0.118s\n",
      "| Adam | epoch: 075 | loss: 1.41175 - acc: 0.3604 -- iter: 15/15\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m1.40881\u001b[0m\u001b[0m | time: 0.121s\n",
      "| Adam | epoch: 076 | loss: 1.40881 - acc: 0.3647 -- iter: 15/15\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m1.40720\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 077 | loss: 1.40720 - acc: 0.3684 -- iter: 15/15\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m1.40364\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 078 | loss: 1.40364 - acc: 0.3717 -- iter: 15/15\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m1.40082\u001b[0m\u001b[0m | time: 0.101s\n",
      "| Adam | epoch: 079 | loss: 1.40082 - acc: 0.3746 -- iter: 15/15\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m1.39418\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 080 | loss: 1.39418 - acc: 0.3772 -- iter: 15/15\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m1.39048\u001b[0m\u001b[0m | time: 0.111s\n",
      "| Adam | epoch: 081 | loss: 1.39048 - acc: 0.3795 -- iter: 15/15\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m1.38851\u001b[0m\u001b[0m | time: 0.106s\n",
      "| Adam | epoch: 082 | loss: 1.38851 - acc: 0.3816 -- iter: 15/15\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m1.38334\u001b[0m\u001b[0m | time: 0.112s\n",
      "| Adam | epoch: 083 | loss: 1.38334 - acc: 0.3834 -- iter: 15/15\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m1.37582\u001b[0m\u001b[0m | time: 0.100s\n",
      "| Adam | epoch: 084 | loss: 1.37582 - acc: 0.3851 -- iter: 15/15\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m1.36975\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 085 | loss: 1.36975 - acc: 0.3866 -- iter: 15/15\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m1.36448\u001b[0m\u001b[0m | time: 0.105s\n",
      "| Adam | epoch: 086 | loss: 1.36448 - acc: 0.3879 -- iter: 15/15\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m1.35721\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 087 | loss: 1.35721 - acc: 0.3891 -- iter: 15/15\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m1.35128\u001b[0m\u001b[0m | time: 0.097s\n",
      "| Adam | epoch: 088 | loss: 1.35128 - acc: 0.3902 -- iter: 15/15\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m1.34256\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 089 | loss: 1.34256 - acc: 0.3912 -- iter: 15/15\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m1.33916\u001b[0m\u001b[0m | time: 0.133s\n",
      "| Adam | epoch: 090 | loss: 1.33916 - acc: 0.3921 -- iter: 15/15\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m1.33318\u001b[0m\u001b[0m | time: 0.109s\n",
      "| Adam | epoch: 091 | loss: 1.33318 - acc: 0.3929 -- iter: 15/15\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m1.32429\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 092 | loss: 1.32429 - acc: 0.4002 -- iter: 15/15\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m1.31252\u001b[0m\u001b[0m | time: 0.100s\n",
      "| Adam | epoch: 093 | loss: 1.31252 - acc: 0.4002 -- iter: 15/15\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m1.32775\u001b[0m\u001b[0m | time: 0.107s\n",
      "| Adam | epoch: 094 | loss: 1.32775 - acc: 0.4002 -- iter: 15/15\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m1.31683\u001b[0m\u001b[0m | time: 0.107s\n",
      "| Adam | epoch: 095 | loss: 1.31683 - acc: 0.4002 -- iter: 15/15\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m1.30346\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 096 | loss: 1.30346 - acc: 0.4002 -- iter: 15/15\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m1.28901\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 097 | loss: 1.28901 - acc: 0.4001 -- iter: 15/15\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m1.27320\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 098 | loss: 1.27320 - acc: 0.4135 -- iter: 15/15\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m1.25830\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 099 | loss: 1.25830 - acc: 0.4255 -- iter: 15/15\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m1.24218\u001b[0m\u001b[0m | time: 0.259s\n",
      "| Adam | epoch: 100 | loss: 1.24218 - acc: 0.4429 -- iter: 15/15\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m1.22603\u001b[0m\u001b[0m | time: 0.106s\n",
      "| Adam | epoch: 101 | loss: 1.22603 - acc: 0.4519 -- iter: 15/15\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m1.21463\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 102 | loss: 1.21463 - acc: 0.4534 -- iter: 15/15\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m1.20185\u001b[0m\u001b[0m | time: 0.101s\n",
      "| Adam | epoch: 103 | loss: 1.20185 - acc: 0.4547 -- iter: 15/15\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m1.18932\u001b[0m\u001b[0m | time: 0.110s\n",
      "| Adam | epoch: 104 | loss: 1.18932 - acc: 0.4626 -- iter: 15/15\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m1.17805\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 105 | loss: 1.17805 - acc: 0.4763 -- iter: 15/15\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m1.16835\u001b[0m\u001b[0m | time: 0.106s\n",
      "| Adam | epoch: 106 | loss: 1.16835 - acc: 0.4887 -- iter: 15/15\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m1.15684\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 107 | loss: 1.15684 - acc: 0.5065 -- iter: 15/15\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m1.14454\u001b[0m\u001b[0m | time: 0.113s\n",
      "| Adam | epoch: 108 | loss: 1.14454 - acc: 0.5225 -- iter: 15/15\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m1.13489\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 109 | loss: 1.13489 - acc: 0.5236 -- iter: 15/15\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m1.19431\u001b[0m\u001b[0m | time: 0.110s\n",
      "| Adam | epoch: 110 | loss: 1.19431 - acc: 0.4979 -- iter: 15/15\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m1.17783\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 111 | loss: 1.17783 - acc: 0.5148 -- iter: 15/15\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m1.15532\u001b[0m\u001b[0m | time: 0.101s\n",
      "| Adam | epoch: 112 | loss: 1.15532 - acc: 0.5366 -- iter: 15/15\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m1.13364\u001b[0m\u001b[0m | time: 0.109s\n",
      "| Adam | epoch: 113 | loss: 1.13364 - acc: 0.5563 -- iter: 15/15\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m1.11759\u001b[0m\u001b[0m | time: 0.101s\n",
      "| Adam | epoch: 114 | loss: 1.11759 - acc: 0.5740 -- iter: 15/15\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m1.09790\u001b[0m\u001b[0m | time: 0.118s\n",
      "| Adam | epoch: 115 | loss: 1.09790 - acc: 0.5966 -- iter: 15/15\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m1.07821\u001b[0m\u001b[0m | time: 0.107s\n",
      "| Adam | epoch: 116 | loss: 1.07821 - acc: 0.6036 -- iter: 15/15\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m1.06596\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 117 | loss: 1.06596 - acc: 0.6099 -- iter: 15/15\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m1.13318\u001b[0m\u001b[0m | time: 0.117s\n",
      "| Adam | epoch: 118 | loss: 1.13318 - acc: 0.5689 -- iter: 15/15\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m1.10968\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 119 | loss: 1.10968 - acc: 0.5787 -- iter: 15/15\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m1.08861\u001b[0m\u001b[0m | time: 0.097s\n",
      "| Adam | epoch: 120 | loss: 1.08861 - acc: 0.5875 -- iter: 15/15\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m1.06798\u001b[0m\u001b[0m | time: 0.106s\n",
      "| Adam | epoch: 121 | loss: 1.06798 - acc: 0.5954 -- iter: 15/15\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m1.04272\u001b[0m\u001b[0m | time: 0.101s\n",
      "| Adam | epoch: 122 | loss: 1.04272 - acc: 0.6092 -- iter: 15/15\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m1.02240\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 123 | loss: 1.02240 - acc: 0.6283 -- iter: 15/15\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m1.00158\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 124 | loss: 1.00158 - acc: 0.6388 -- iter: 15/15\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.98238\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 125 | loss: 0.98238 - acc: 0.6416 -- iter: 15/15\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.96396\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 126 | loss: 0.96396 - acc: 0.6441 -- iter: 15/15\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.94874\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 127 | loss: 0.94874 - acc: 0.6463 -- iter: 15/15\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.93216\u001b[0m\u001b[0m | time: 0.124s\n",
      "| Adam | epoch: 128 | loss: 0.93216 - acc: 0.6550 -- iter: 15/15\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.91364\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 129 | loss: 0.91364 - acc: 0.6629 -- iter: 15/15\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.89569\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 130 | loss: 0.89569 - acc: 0.6699 -- iter: 15/15\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.87606\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 131 | loss: 0.87606 - acc: 0.6696 -- iter: 15/15\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.86706\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 132 | loss: 0.86706 - acc: 0.6760 -- iter: 15/15\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.85498\u001b[0m\u001b[0m | time: 0.107s\n",
      "| Adam | epoch: 133 | loss: 0.85498 - acc: 0.6750 -- iter: 15/15\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.83728\u001b[0m\u001b[0m | time: 0.126s\n",
      "| Adam | epoch: 134 | loss: 0.83728 - acc: 0.6742 -- iter: 15/15\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.81714\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 135 | loss: 0.81714 - acc: 0.6868 -- iter: 15/15\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.80108\u001b[0m\u001b[0m | time: 0.105s\n",
      "| Adam | epoch: 136 | loss: 0.80108 - acc: 0.6848 -- iter: 15/15\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.78181\u001b[0m\u001b[0m | time: 0.120s\n",
      "| Adam | epoch: 137 | loss: 0.78181 - acc: 0.6896 -- iter: 15/15\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.76228\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 138 | loss: 0.76228 - acc: 0.7007 -- iter: 15/15\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.74550\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 139 | loss: 0.74550 - acc: 0.7106 -- iter: 15/15\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.73049\u001b[0m\u001b[0m | time: 0.097s\n",
      "| Adam | epoch: 140 | loss: 0.73049 - acc: 0.7129 -- iter: 15/15\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.71608\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 141 | loss: 0.71608 - acc: 0.7149 -- iter: 15/15\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.70270\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 142 | loss: 0.70270 - acc: 0.7168 -- iter: 15/15\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.68241\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 143 | loss: 0.68241 - acc: 0.7251 -- iter: 15/15\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.66731\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 144 | loss: 0.66731 - acc: 0.7192 -- iter: 15/15\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.64980\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 145 | loss: 0.64980 - acc: 0.7273 -- iter: 15/15\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.63434\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 146 | loss: 0.63434 - acc: 0.7346 -- iter: 15/15\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.61713\u001b[0m\u001b[0m | time: 0.121s\n",
      "| Adam | epoch: 147 | loss: 0.61713 - acc: 0.7411 -- iter: 15/15\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.60294\u001b[0m\u001b[0m | time: 0.088s\n",
      "| Adam | epoch: 148 | loss: 0.60294 - acc: 0.7470 -- iter: 15/15\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.58897\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 149 | loss: 0.58897 - acc: 0.7456 -- iter: 15/15\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.57290\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 150 | loss: 0.57290 - acc: 0.7511 -- iter: 15/15\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.55571\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 151 | loss: 0.55571 - acc: 0.7693 -- iter: 15/15\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.54479\u001b[0m\u001b[0m | time: 0.109s\n",
      "| Adam | epoch: 152 | loss: 0.54479 - acc: 0.7724 -- iter: 15/15\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.52950\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 153 | loss: 0.52950 - acc: 0.7885 -- iter: 15/15\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.51816\u001b[0m\u001b[0m | time: 0.090s\n",
      "| Adam | epoch: 154 | loss: 0.51816 - acc: 0.7896 -- iter: 15/15\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.50856\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 155 | loss: 0.50856 - acc: 0.7907 -- iter: 15/15\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.49531\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 156 | loss: 0.49531 - acc: 0.8116 -- iter: 15/15\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.48782\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 157 | loss: 0.48782 - acc: 0.8171 -- iter: 15/15\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.47703\u001b[0m\u001b[0m | time: 0.107s\n",
      "| Adam | epoch: 158 | loss: 0.47703 - acc: 0.8221 -- iter: 15/15\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.46558\u001b[0m\u001b[0m | time: 0.086s\n",
      "| Adam | epoch: 159 | loss: 0.46558 - acc: 0.8265 -- iter: 15/15\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.45557\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 160 | loss: 0.45557 - acc: 0.8305 -- iter: 15/15\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.44601\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 161 | loss: 0.44601 - acc: 0.8341 -- iter: 15/15\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.44406\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 162 | loss: 0.44406 - acc: 0.8374 -- iter: 15/15\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.43256\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 163 | loss: 0.43256 - acc: 0.8537 -- iter: 15/15\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.43001\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 164 | loss: 0.43001 - acc: 0.8550 -- iter: 15/15\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.42059\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 165 | loss: 0.42059 - acc: 0.8561 -- iter: 15/15\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.41313\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 166 | loss: 0.41313 - acc: 0.8505 -- iter: 15/15\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.40473\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 167 | loss: 0.40473 - acc: 0.8455 -- iter: 15/15\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.39657\u001b[0m\u001b[0m | time: 0.109s\n",
      "| Adam | epoch: 168 | loss: 0.39657 - acc: 0.8476 -- iter: 15/15\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.38836\u001b[0m\u001b[0m | time: 0.090s\n",
      "| Adam | epoch: 169 | loss: 0.38836 - acc: 0.8495 -- iter: 15/15\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.38220\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 170 | loss: 0.38220 - acc: 0.8512 -- iter: 15/15\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.37479\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 171 | loss: 0.37479 - acc: 0.8594 -- iter: 15/15\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.36313\u001b[0m\u001b[0m | time: 0.097s\n",
      "| Adam | epoch: 172 | loss: 0.36313 - acc: 0.8668 -- iter: 15/15\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.35172\u001b[0m\u001b[0m | time: 0.101s\n",
      "| Adam | epoch: 173 | loss: 0.35172 - acc: 0.8735 -- iter: 15/15\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.34227\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 174 | loss: 0.34227 - acc: 0.8795 -- iter: 15/15\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.33146\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 175 | loss: 0.33146 - acc: 0.8848 -- iter: 15/15\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.32306\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 176 | loss: 0.32306 - acc: 0.8897 -- iter: 15/15\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.31699\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 177 | loss: 0.31699 - acc: 0.8941 -- iter: 15/15\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.30608\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 178 | loss: 0.30608 - acc: 0.8980 -- iter: 15/15\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.29900\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 179 | loss: 0.29900 - acc: 0.9015 -- iter: 15/15\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.29251\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 180 | loss: 0.29251 - acc: 0.9047 -- iter: 15/15\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.28687\u001b[0m\u001b[0m | time: 0.165s\n",
      "| Adam | epoch: 181 | loss: 0.28687 - acc: 0.9076 -- iter: 15/15\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.27923\u001b[0m\u001b[0m | time: 0.115s\n",
      "| Adam | epoch: 182 | loss: 0.27923 - acc: 0.9101 -- iter: 15/15\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.27045\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 183 | loss: 0.27045 - acc: 0.9125 -- iter: 15/15\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.26257\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 184 | loss: 0.26257 - acc: 0.9145 -- iter: 15/15\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.25712\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 185 | loss: 0.25712 - acc: 0.9164 -- iter: 15/15\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.25406\u001b[0m\u001b[0m | time: 0.115s\n",
      "| Adam | epoch: 186 | loss: 0.25406 - acc: 0.9181 -- iter: 15/15\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.24587\u001b[0m\u001b[0m | time: 0.150s\n",
      "| Adam | epoch: 187 | loss: 0.24587 - acc: 0.9263 -- iter: 15/15\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.24382\u001b[0m\u001b[0m | time: 0.101s\n",
      "| Adam | epoch: 188 | loss: 0.24382 - acc: 0.9270 -- iter: 15/15\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.24134\u001b[0m\u001b[0m | time: 0.088s\n",
      "| Adam | epoch: 189 | loss: 0.24134 - acc: 0.9276 -- iter: 15/15\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.23775\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 190 | loss: 0.23775 - acc: 0.9282 -- iter: 15/15\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.23368\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 191 | loss: 0.23368 - acc: 0.9221 -- iter: 15/15\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.22785\u001b[0m\u001b[0m | time: 0.097s\n",
      "| Adam | epoch: 192 | loss: 0.22785 - acc: 0.9232 -- iter: 15/15\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.22791\u001b[0m\u001b[0m | time: 0.135s\n",
      "| Adam | epoch: 193 | loss: 0.22791 - acc: 0.9242 -- iter: 15/15\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.22412\u001b[0m\u001b[0m | time: 0.154s\n",
      "| Adam | epoch: 194 | loss: 0.22412 - acc: 0.9251 -- iter: 15/15\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.22008\u001b[0m\u001b[0m | time: 0.131s\n",
      "| Adam | epoch: 195 | loss: 0.22008 - acc: 0.9259 -- iter: 15/15\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.21478\u001b[0m\u001b[0m | time: 0.205s\n",
      "| Adam | epoch: 196 | loss: 0.21478 - acc: 0.9267 -- iter: 15/15\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.21163\u001b[0m\u001b[0m | time: 0.130s\n",
      "| Adam | epoch: 197 | loss: 0.21163 - acc: 0.9273 -- iter: 15/15\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.20525\u001b[0m\u001b[0m | time: 0.109s\n",
      "| Adam | epoch: 198 | loss: 0.20525 - acc: 0.9279 -- iter: 15/15\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.20312\u001b[0m\u001b[0m | time: 0.107s\n",
      "| Adam | epoch: 199 | loss: 0.20312 - acc: 0.9285 -- iter: 15/15\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.20189\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 200 | loss: 0.20189 - acc: 0.9290 -- iter: 15/15\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.19904\u001b[0m\u001b[0m | time: 0.104s\n",
      "| Adam | epoch: 201 | loss: 0.19904 - acc: 0.9294 -- iter: 15/15\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.19905\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 202 | loss: 0.19905 - acc: 0.9298 -- iter: 15/15\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.19574\u001b[0m\u001b[0m | time: 0.114s\n",
      "| Adam | epoch: 203 | loss: 0.19574 - acc: 0.9301 -- iter: 15/15\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.19319\u001b[0m\u001b[0m | time: 0.163s\n",
      "| Adam | epoch: 204 | loss: 0.19319 - acc: 0.9305 -- iter: 15/15\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.19286\u001b[0m\u001b[0m | time: 0.105s\n",
      "| Adam | epoch: 205 | loss: 0.19286 - acc: 0.9308 -- iter: 15/15\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.18713\u001b[0m\u001b[0m | time: 0.131s\n",
      "| Adam | epoch: 206 | loss: 0.18713 - acc: 0.9310 -- iter: 15/15\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.18409\u001b[0m\u001b[0m | time: 0.138s\n",
      "| Adam | epoch: 207 | loss: 0.18409 - acc: 0.9312 -- iter: 15/15\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.18349\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 208 | loss: 0.18349 - acc: 0.9315 -- iter: 15/15\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.18457\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 209 | loss: 0.18457 - acc: 0.9316 -- iter: 15/15\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.18046\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 210 | loss: 0.18046 - acc: 0.9318 -- iter: 15/15\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.17782\u001b[0m\u001b[0m | time: 0.169s\n",
      "| Adam | epoch: 211 | loss: 0.17782 - acc: 0.9320 -- iter: 15/15\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.17669\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 212 | loss: 0.17669 - acc: 0.9321 -- iter: 15/15\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.17346\u001b[0m\u001b[0m | time: 0.147s\n",
      "| Adam | epoch: 213 | loss: 0.17346 - acc: 0.9322 -- iter: 15/15\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.17458\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 214 | loss: 0.17458 - acc: 0.9323 -- iter: 15/15\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.17249\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 215 | loss: 0.17249 - acc: 0.9324 -- iter: 15/15\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.17370\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 216 | loss: 0.17370 - acc: 0.9259 -- iter: 15/15\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.17122\u001b[0m\u001b[0m | time: 0.085s\n",
      "| Adam | epoch: 217 | loss: 0.17122 - acc: 0.9266 -- iter: 15/15\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.17160\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 218 | loss: 0.17160 - acc: 0.9273 -- iter: 15/15\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.16879\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 219 | loss: 0.16879 - acc: 0.9279 -- iter: 15/15\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.16926\u001b[0m\u001b[0m | time: 0.097s\n",
      "| Adam | epoch: 220 | loss: 0.16926 - acc: 0.9284 -- iter: 15/15\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.16421\u001b[0m\u001b[0m | time: 0.100s\n",
      "| Adam | epoch: 221 | loss: 0.16421 - acc: 0.9289 -- iter: 15/15\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.16283\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 222 | loss: 0.16283 - acc: 0.9294 -- iter: 15/15\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.16326\u001b[0m\u001b[0m | time: 0.101s\n",
      "| Adam | epoch: 223 | loss: 0.16326 - acc: 0.9298 -- iter: 15/15\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.16513\u001b[0m\u001b[0m | time: 0.090s\n",
      "| Adam | epoch: 224 | loss: 0.16513 - acc: 0.9301 -- iter: 15/15\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.16653\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 225 | loss: 0.16653 - acc: 0.9304 -- iter: 15/15\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.16435\u001b[0m\u001b[0m | time: 0.110s\n",
      "| Adam | epoch: 226 | loss: 0.16435 - acc: 0.9307 -- iter: 15/15\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.16268\u001b[0m\u001b[0m | time: 0.099s\n",
      "| Adam | epoch: 227 | loss: 0.16268 - acc: 0.9310 -- iter: 15/15\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.16096\u001b[0m\u001b[0m | time: 0.100s\n",
      "| Adam | epoch: 228 | loss: 0.16096 - acc: 0.9246 -- iter: 15/15\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.16340\u001b[0m\u001b[0m | time: 0.100s\n",
      "| Adam | epoch: 229 | loss: 0.16340 - acc: 0.9188 -- iter: 15/15\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.16152\u001b[0m\u001b[0m | time: 0.090s\n",
      "| Adam | epoch: 230 | loss: 0.16152 - acc: 0.9202 -- iter: 15/15\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.15996\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 231 | loss: 0.15996 - acc: 0.9215 -- iter: 15/15\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.16096\u001b[0m\u001b[0m | time: 0.121s\n",
      "| Adam | epoch: 232 | loss: 0.16096 - acc: 0.9227 -- iter: 15/15\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.16079\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 233 | loss: 0.16079 - acc: 0.9238 -- iter: 15/15\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.16259\u001b[0m\u001b[0m | time: 0.087s\n",
      "| Adam | epoch: 234 | loss: 0.16259 - acc: 0.9181 -- iter: 15/15\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.16036\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 235 | loss: 0.16036 - acc: 0.9196 -- iter: 15/15\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.16054\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 236 | loss: 0.16054 - acc: 0.9210 -- iter: 15/15\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.15854\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 237 | loss: 0.15854 - acc: 0.9222 -- iter: 15/15\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.15562\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 238 | loss: 0.15562 - acc: 0.9233 -- iter: 15/15\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.15823\u001b[0m\u001b[0m | time: 0.088s\n",
      "| Adam | epoch: 239 | loss: 0.15823 - acc: 0.9243 -- iter: 15/15\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.15480\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 240 | loss: 0.15480 - acc: 0.9252 -- iter: 15/15\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.15461\u001b[0m\u001b[0m | time: 0.081s\n",
      "| Adam | epoch: 241 | loss: 0.15461 - acc: 0.9260 -- iter: 15/15\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.15039\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 242 | loss: 0.15039 - acc: 0.9334 -- iter: 15/15\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.14736\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 243 | loss: 0.14736 - acc: 0.9401 -- iter: 15/15\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.14525\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 244 | loss: 0.14525 - acc: 0.9394 -- iter: 15/15\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.14364\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 245 | loss: 0.14364 - acc: 0.9388 -- iter: 15/15\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.14531\u001b[0m\u001b[0m | time: 0.088s\n",
      "| Adam | epoch: 246 | loss: 0.14531 - acc: 0.9383 -- iter: 15/15\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.14444\u001b[0m\u001b[0m | time: 0.109s\n",
      "| Adam | epoch: 247 | loss: 0.14444 - acc: 0.9378 -- iter: 15/15\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.14192\u001b[0m\u001b[0m | time: 0.088s\n",
      "| Adam | epoch: 248 | loss: 0.14192 - acc: 0.9373 -- iter: 15/15\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.14284\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 249 | loss: 0.14284 - acc: 0.9369 -- iter: 15/15\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.14218\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 250 | loss: 0.14218 - acc: 0.9366 -- iter: 15/15\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.13959\u001b[0m\u001b[0m | time: 0.087s\n",
      "| Adam | epoch: 251 | loss: 0.13959 - acc: 0.9429 -- iter: 15/15\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.13816\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 252 | loss: 0.13816 - acc: 0.9419 -- iter: 15/15\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.13495\u001b[0m\u001b[0m | time: 0.090s\n",
      "| Adam | epoch: 253 | loss: 0.13495 - acc: 0.9411 -- iter: 15/15\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.13714\u001b[0m\u001b[0m | time: 0.100s\n",
      "| Adam | epoch: 254 | loss: 0.13714 - acc: 0.9403 -- iter: 15/15\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.13655\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 255 | loss: 0.13655 - acc: 0.9396 -- iter: 15/15\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.13448\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 256 | loss: 0.13448 - acc: 0.9390 -- iter: 15/15\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.13086\u001b[0m\u001b[0m | time: 0.106s\n",
      "| Adam | epoch: 257 | loss: 0.13086 - acc: 0.9451 -- iter: 15/15\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.12905\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 258 | loss: 0.12905 - acc: 0.9506 -- iter: 15/15\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.12886\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 259 | loss: 0.12886 - acc: 0.9489 -- iter: 15/15\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.13122\u001b[0m\u001b[0m | time: 0.085s\n",
      "| Adam | epoch: 260 | loss: 0.13122 - acc: 0.9473 -- iter: 15/15\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.13073\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 261 | loss: 0.13073 - acc: 0.9459 -- iter: 15/15\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.60736\u001b[0m\u001b[0m | time: 0.093s\n",
      "| Adam | epoch: 262 | loss: 0.60736 - acc: 0.8580 -- iter: 15/15\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.59565\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 263 | loss: 0.59565 - acc: 0.8522 -- iter: 15/15\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.57307\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 264 | loss: 0.57307 - acc: 0.8536 -- iter: 15/15\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.52575\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 265 | loss: 0.52575 - acc: 0.8683 -- iter: 15/15\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.48666\u001b[0m\u001b[0m | time: 0.114s\n",
      "| Adam | epoch: 266 | loss: 0.48666 - acc: 0.8814 -- iter: 15/15\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.45691\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 267 | loss: 0.45691 - acc: 0.8866 -- iter: 15/15\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.42477\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 268 | loss: 0.42477 - acc: 0.8913 -- iter: 15/15\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.39582\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 269 | loss: 0.39582 - acc: 0.8955 -- iter: 15/15\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.37262\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 270 | loss: 0.37262 - acc: 0.8993 -- iter: 15/15\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.34436\u001b[0m\u001b[0m | time: 0.095s\n",
      "| Adam | epoch: 271 | loss: 0.34436 - acc: 0.9094 -- iter: 15/15\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.32275\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 272 | loss: 0.32275 - acc: 0.9184 -- iter: 15/15\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.30224\u001b[0m\u001b[0m | time: 0.102s\n",
      "| Adam | epoch: 273 | loss: 0.30224 - acc: 0.9199 -- iter: 15/15\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.28525\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 274 | loss: 0.28525 - acc: 0.9279 -- iter: 15/15\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.27314\u001b[0m\u001b[0m | time: 0.108s\n",
      "| Adam | epoch: 275 | loss: 0.27314 - acc: 0.9351 -- iter: 15/15\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.63916\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 276 | loss: 0.63916 - acc: 0.8683 -- iter: 15/15\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.59409\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 277 | loss: 0.59409 - acc: 0.8748 -- iter: 15/15\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.55919\u001b[0m\u001b[0m | time: 0.107s\n",
      "| Adam | epoch: 278 | loss: 0.55919 - acc: 0.8806 -- iter: 15/15\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.52742\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 279 | loss: 0.52742 - acc: 0.8859 -- iter: 15/15\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.49691\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 280 | loss: 0.49691 - acc: 0.8840 -- iter: 15/15\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.47001\u001b[0m\u001b[0m | time: 0.101s\n",
      "| Adam | epoch: 281 | loss: 0.47001 - acc: 0.8823 -- iter: 15/15\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.44536\u001b[0m\u001b[0m | time: 0.094s\n",
      "| Adam | epoch: 282 | loss: 0.44536 - acc: 0.8807 -- iter: 15/15\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.41603\u001b[0m\u001b[0m | time: 0.103s\n",
      "| Adam | epoch: 283 | loss: 0.41603 - acc: 0.8860 -- iter: 15/15\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.39004\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 284 | loss: 0.39004 - acc: 0.8907 -- iter: 15/15\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.36521\u001b[0m\u001b[0m | time: 0.092s\n",
      "| Adam | epoch: 285 | loss: 0.36521 - acc: 0.8950 -- iter: 15/15\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.34404\u001b[0m\u001b[0m | time: 0.096s\n",
      "| Adam | epoch: 286 | loss: 0.34404 - acc: 0.8988 -- iter: 15/15\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.32342\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 287 | loss: 0.32342 - acc: 0.9023 -- iter: 15/15\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.30520\u001b[0m\u001b[0m | time: 0.101s\n",
      "| Adam | epoch: 288 | loss: 0.30520 - acc: 0.9120 -- iter: 15/15\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.28759\u001b[0m\u001b[0m | time: 0.089s\n",
      "| Adam | epoch: 289 | loss: 0.28759 - acc: 0.9208 -- iter: 15/15\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.26996\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 290 | loss: 0.26996 - acc: 0.9287 -- iter: 15/15\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.25467\u001b[0m\u001b[0m | time: 0.091s\n",
      "| Adam | epoch: 291 | loss: 0.25467 - acc: 0.9359 -- iter: 15/15\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.24165\u001b[0m\u001b[0m | time: 0.082s\n",
      "| Adam | epoch: 292 | loss: 0.24165 - acc: 0.9423 -- iter: 15/15\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.22941\u001b[0m\u001b[0m | time: 0.088s\n",
      "| Adam | epoch: 293 | loss: 0.22941 - acc: 0.9481 -- iter: 15/15\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.21806\u001b[0m\u001b[0m | time: 0.098s\n",
      "| Adam | epoch: 294 | loss: 0.21806 - acc: 0.9532 -- iter: 15/15\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.20443\u001b[0m\u001b[0m | time: 0.088s\n",
      "| Adam | epoch: 295 | loss: 0.20443 - acc: 0.9579 -- iter: 15/15\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.19275\u001b[0m\u001b[0m | time: 0.083s\n",
      "| Adam | epoch: 296 | loss: 0.19275 - acc: 0.9621 -- iter: 15/15\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.18240\u001b[0m\u001b[0m | time: 0.224s\n",
      "| Adam | epoch: 297 | loss: 0.18240 - acc: 0.9659 -- iter: 15/15\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.17306\u001b[0m\u001b[0m | time: 0.100s\n",
      "| Adam | epoch: 298 | loss: 0.17306 - acc: 0.9693 -- iter: 15/15\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.16446\u001b[0m\u001b[0m | time: 0.084s\n",
      "| Adam | epoch: 299 | loss: 0.16446 - acc: 0.9724 -- iter: 15/15\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.15740\u001b[0m\u001b[0m | time: 0.087s\n",
      "| Adam | epoch: 300 | loss: 0.15740 - acc: 0.9752 -- iter: 15/15\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model = tflearn.DNN(net)\n",
    "history = model.fit(training, output, n_epoch=300, batch_size=128, show_metric=True, run_id='intents')\n",
    "#model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "\n",
    "    return numpy.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    print(\"Comece a conversar com o bot (digite quit para parar)!\")\n",
    "    while True:\n",
    "        inp = input(\"Você: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)])[0]\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        if results[results_index] > 0.7:\n",
    "            for tg in data[\"intents\"]:\n",
    "                if tg[\"tag\"] == tag:\n",
    "                    responses = tg[\"responses\"]\n",
    "\n",
    "            print(random.choice(responses))\n",
    "        else:\n",
    "            print(\"Não entendi, tente novamente. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comece a conversar com o bot (digite quit para parar)!\n",
      "Você: ola\n",
      "Não entendi, tente novamente. \n",
      "Você: ola\n",
      "Não entendi, tente novamente. \n",
      "Você: olá\n",
      "Olá, bem-vindo ao nosso Chatbot\n",
      "Você: tudo bem?\n",
      "Não entendi, tente novamente. \n",
      "Você: pagar o ipva\n",
      "O prazer é meu! Posso ajudar com algum outro problema?\n",
      "Você: como faço para pagar o ipva\n",
      "Os valores separados do IPVA, da Taxa de Licenciamento Anual e do seguro obrigatório podem ser obtidos nesta página da Internet (www.receita.fazenda.df.gov.br) em: Menu Receita / Serviços Cidadão / Veículos, mediante o fornecimento do nº do RENAVAM ou por meio do telefone “156”, opção “3”, ou em uma das unidades do “Na Hora”, ou em uma das Agências/Posto de Atendimento da Receita, mediante o fornecimento do nº da placa e do RENAVAM.\n",
      "Você: como pagar o ipva anos anteriores\n",
      "Os valores pagos e a pagar podem ser obtidos nesta página da Internet (www.receita.fazenda.df.gov.br) em: Menu Receita / Serviços Cidadão / Veículos, ou por meio do telefone “156”, opção “3”, mediante o fornecimento do nº da placa e do RENAVAM, ou em uma das Agências/Posto de Atendimento da Receita, por meio da indicação do RENAVAM ou documentos que comprovem ser o proprietário do veículo ou procurador deste (CRLV, documento de identidade, CPF e procuração, se for o caso).\n"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
